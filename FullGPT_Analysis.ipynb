{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MaFPyHYRNR5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import textstat\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqKPnDxiSHRn",
        "outputId": "ead31357-495f-4310-a680-2541cbef489a"
      },
      "outputs": [],
      "source": [
        "df_orig = pd.read_csv('./full_gpt_2.csv')\n",
        "df_orig.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P5QNuWMdqvn",
        "outputId": "795a0736-3edf-4960-cb80-bb3e5133788a"
      },
      "outputs": [],
      "source": [
        "#create a copy of the original dataframe\n",
        "df = df_orig.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNqo_qj5XiLb",
        "outputId": "e7e61f55-a956-4fdb-bcbb-2e61651a00db"
      },
      "outputs": [],
      "source": [
        "df = df.iloc[:,1:]\n",
        "df.dropna(inplace=True)\n",
        "#df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4ADE8Hfdqvo"
      },
      "source": [
        "# Grouping status type\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guAM1HVFUpK0",
        "outputId": "7e240e3f-1ac4-4da2-8488-8f2c7f7fdb85"
      },
      "outputs": [],
      "source": [
        "df['Status'] = df['Status'].apply(lambda x: x.lower())\n",
        "#df['Status'] = df['Status'].apply(lambda x: 'ER' if re.search('memory.*limit', x) else x)\n",
        "#df['Status'] = df['Status'].apply(lambda x: 'ER' if re.search('time.*limit', x) else x)\n",
        "df['Status'] = df['Status'].apply(lambda x: 'ER' if re.search('run.*time', x) else x)\n",
        "df['Status'] = df['Status'].apply(lambda x: 'WA' if re.search('wrong.*answer', x) else x)\n",
        "df['Status'] = df['Status'].apply(lambda x: 'AC' if re.search('accepted', x) else x)\n",
        "df['Status'] = df['Status'].apply(lambda x: 'MLE' if re.search('memory.*limit', x) else x)\n",
        "df['Status'] = df['Status'].apply(lambda x: 'TLE' if re.search('time.*limit', x) else x)\n",
        "#df['Status'] = df['Status'].apply(lambda x: 'RTE' if re.search('run.*time', x) else x)\n",
        "df['Status'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_vgqcwhdqvq"
      },
      "source": [
        "# Textstat Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMO466xDYNk0"
      },
      "outputs": [],
      "source": [
        "#'DCR','textStd','FKG','SMOG','CLI','DFFW','LINSEAR','GF'\n",
        "#Using TextStat to calculate readability scores\n",
        "df['ARI'] = df['Description'].apply(lambda x: textstat.automated_readability_index(x))\n",
        "df['DCR'] = df['Description'].apply(lambda x: textstat.dale_chall_readability_score(x))\n",
        "# the Flesch Reading Ease Score\n",
        "df['FRE'] = df['Description'].apply(lambda x: textstat.flesch_reading_ease(x))\n",
        "# Sentence Count\n",
        "df['SenCount'] = df['Description'].apply(lambda x: textstat.sentence_count(x))\n",
        "# Lexicon Count\n",
        "df['LexCount'] = df['Description'].apply(lambda x: textstat.lexicon_count(x))\n",
        "# Readability Consensus based upon all the above tests (text standard)\n",
        "df['textStd'] = df['Description'].apply(lambda x: textstat.text_standard(x, float_output=True))\n",
        "df['FKG'] = df['Description'].apply(lambda x: textstat.flesch_kincaid_grade(x))\n",
        "df['SMOG'] = df['Description'].apply(lambda x: textstat.smog_index(x))\n",
        "df['CLI'] = df['Description'].apply(lambda x: textstat.coleman_liau_index(x))\n",
        "df['DFFW'] = df['Description'].apply(lambda x: textstat.difficult_words(x))\n",
        "df['LINSEAR'] = df['Description'].apply(lambda x: textstat.linsear_write_formula(x))\n",
        "df['GF'] = df['Description'].apply(lambda x: textstat.gunning_fog(x))\n",
        "# Test_diff = Total - Pass\n",
        "df['Unpass'] = df['Total'] - df['Pass']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckBnABKgdqvq"
      },
      "source": [
        "# Tokenization Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EQNtUKudqvq",
        "outputId": "522b98e4-933c-41c8-cc16-55310bcadb8b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize nltk resources\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return words\n",
        "\n",
        "df_w2v = df.copy()\n",
        "\n",
        "# Word Embeddings using Word2Vec\n",
        "df_w2v['tokens'] = [preprocess_text(doc) for doc in df_w2v.Description]\n",
        "\n",
        "wv_model = Word2Vec(df_w2v['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "df_w2v['word2vec'] = df_w2v['tokens'].apply(lambda tokens: np.mean([wv_model.wv[token] for token in tokens if token in wv_model.wv], axis=0))\n",
        "\n",
        "# Split the Word2Vec features into separate columns\n",
        "word2vecDFrame = pd.DataFrame(df_w2v['word2vec'].to_list(), columns=[f'word2vec_{i}' for i in range(100)])\n",
        "df_w2v = df_w2v.drop(['tokens','word2vec'], axis = 1)\n",
        "df_w2v.reset_index(drop=True, inplace=True)\n",
        "word2vecDFrame.reset_index(drop=True, inplace=True)\n",
        "df_w2v = pd.merge(df_w2v, word2vecDFrame, left_index=True, right_index=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MFfFV5wdqvr"
      },
      "source": [
        "# Reversed engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv01HgEFdqvr",
        "outputId": "6408fc6d-f99b-4956-cb61-d0c5bd877b24"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Load the previously trained model\n",
        "model = wv_model\n",
        "\n",
        "# Create a list of column names\n",
        "column_names = ['word2vec_' + str(i) for i in range(0,100)]\n",
        "\n",
        "for index, row in df_w2v[column_names].iterrows():\n",
        "    #print(row)\n",
        "    similar_words = model.wv.similar_by_vector(row.values, topn=20)\n",
        "    if (index == 93):\n",
        "        print(similar_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fPhRVAUdqvr"
      },
      "source": [
        "# Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mtt6An-Zdqvr"
      },
      "outputs": [],
      "source": [
        "# Word Frequency: Bag of Words\n",
        "vectorizer = CountVectorizer(analyzer=preprocess_text)\n",
        "bow_matrix = vectorizer.fit_transform(df.Description)\n",
        "bow_matrix_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Sum the counts of each word across all documents\n",
        "word_counts = bow_matrix_df.sum(axis=0)\n",
        "\n",
        "# Sort the words by their counts in descending order\n",
        "sorted_word_counts = word_counts.sort_values(ascending=False)\n",
        "\n",
        "# Select the top N words\n",
        "N = 100  \n",
        "top_words = sorted_word_counts[:N].index\n",
        "\n",
        "top_words_bow_df = bow_matrix_df[top_words]\n",
        "\n",
        "#print(top_words)\n",
        "df_bow = df.copy()\n",
        "\n",
        "df_bow.reset_index(drop=True, inplace=True)\n",
        "top_words_bow_df.reset_index(drop=True, inplace=True)\n",
        "df_bow = pd.merge(df_bow, top_words_bow_df, left_index=True, right_index=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxcwHFFmdqvr"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H95CFb1Vdqvr",
        "outputId": "ca425895-bc84-48d3-938a-575bcfc8e83c"
      },
      "outputs": [],
      "source": [
        "# Term frequency: TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(analyzer=preprocess_text)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df.Description)\n",
        "tfidf_matrix_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# For each training sample, get the top N features\n",
        "N = 100  \n",
        "top_features = tfidf_matrix_df.apply(lambda row: row.nlargest(N).index, axis=1)\n",
        "\n",
        "#print(top_features)\n",
        "\n",
        "# Convert the top features into a DataFrame\n",
        "top_features_df = pd.DataFrame(top_features.tolist(), columns=[f'top_{i+1}' for i in range(N)])\n",
        "\n",
        "top_features_scores_tfidf_df = pd.DataFrame()\n",
        "\n",
        "# For each document\n",
        "for i in range(len(top_features_df)):\n",
        "    # For each of the top N words\n",
        "    for j in range(N):\n",
        "        word = top_features_df.iloc[i, j]\n",
        "        score = tfidf_matrix_df.loc[i, word]\n",
        "        top_features_scores_tfidf_df.loc[i, f'top_{j+1}'] = score\n",
        "\n",
        "#print(top_features_scores_df)\n",
        "\n",
        "df_tfidf = df.copy()\n",
        "df_tfidf.reset_index(drop=True, inplace=True)\n",
        "top_features_scores_tfidf_df.reset_index(drop=True, inplace=True)\n",
        "df_tfidf = pd.merge(df_tfidf, top_features_scores_tfidf_df, left_index=True, right_index=True)\n",
        "#df_tfidf.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mXSt8nYdqvs"
      },
      "source": [
        "# All tokenized features into one DF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ySzT_RSdqvs",
        "outputId": "81836f72-3598-4ea2-e9f8-89f6450936e7"
      },
      "outputs": [],
      "source": [
        "df_tokenized = df.copy()\n",
        "#df_tokenized = pd.concat([df_tokenized, word2vecDFrame, top_words_bow_df, top_features_scores_tfidf_df], axis=1)\n",
        "df_tokenized.reset_index(drop=True, inplace=True)\n",
        "word2vecDFrame.reset_index(drop=True, inplace=True)\n",
        "top_words_bow_df.reset_index(drop=True, inplace=True)\n",
        "top_features_scores_tfidf_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#df_tokenized = pd.merge(df_tokenized, word2vecDFrame, top_words_bow_df, top_features_scores_tfidf_df, left_index=True, right_index=True)\n",
        "df_tokenized = pd.concat([df_tokenized, word2vecDFrame, top_words_bow_df, top_features_scores_tfidf_df], axis=1)\n",
        "df_tokenized.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a_Re_ilEGM4"
      },
      "source": [
        "# Status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "4KLME7lzEJMB",
        "outputId": "63085c49-19c3-4f80-aa26-ec64c6162e6f"
      },
      "outputs": [],
      "source": [
        "g = sns.displot(data=df, x=\"Status\", height=5, aspect=1)\n",
        "g.set(title ='Histogram of Status')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmdpVdE_FJbV"
      },
      "source": [
        "# Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1RmvgjXdqvz",
        "outputId": "c063abf8-54a2-4bab-a908-05a88101e4f0"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "df_encode = df.copy()\n",
        "le = preprocessing.LabelEncoder()\n",
        "df_encode.drop(columns=['Problem','Description'], inplace=True)\n",
        "#encode Status [Accepted, Error, Wrong Answer] to [0,1,2]\n",
        "df_encode['Status'] = le.fit_transform(df_encode['Status'])\n",
        "df_encode['Status'].value_counts()\n",
        "#df_encode.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E2CwLi41Vyte",
        "outputId": "71432ee0-e883-4393-adef-5aeb7070606b"
      },
      "outputs": [],
      "source": [
        "sns.set(rc={'figure.figsize':[20,10]}, font_scale=1.25)\n",
        "sns.heatmap(df_encode.corr(), cmap=\"YlGnBu\", annot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4mv2u3QG0-J"
      },
      "source": [
        "# Unpass tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqIkcd4Ddqvz",
        "outputId": "3e3cb67f-1e70-4c79-cb9d-b9b16eff6845"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(df_encode, hue='Status')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NLDB9C_dqv0"
      },
      "source": [
        "# Prepare train and test data, using only textstat features\n",
        "\n",
        "Status is ungrouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6nWBN30dqv0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "\n",
        "train, test = train_test_split(df_encode, test_size=0.2, stratify=df_encode['Status'].values)\n",
        "\n",
        "# Status\n",
        "# 4 wa     1945\n",
        "# 2 rte     817\n",
        "# 0 ac      386\n",
        "# 3 tle     146\n",
        "# 1 mle      29\n",
        "\n",
        "#upsampling the minority classes Accepted and Errors\n",
        "ac = train[train.Status==0]\n",
        "mle = train[train.Status==1]\n",
        "rte = train[train.Status==2]\n",
        "tle = train[train.Status==3]\n",
        "wa = train[train.Status==4]\n",
        "\n",
        "ac_upsampled = resample(ac, replace=True, # sample with replacement\n",
        "                        n_samples=len(wa)) # match number in majority class\n",
        "                        \n",
        "\n",
        "mle_upsampled = resample(mle, replace=True,\n",
        "                        n_samples=len(wa))\n",
        "                        \n",
        "\n",
        "tle_upsampled = resample(tle, replace=True,\n",
        "                        n_samples=len(wa))\n",
        "                    \n",
        "\n",
        "rte_upsampled = resample(rte, replace=True,\n",
        "                        n_samples=len(wa))\n",
        "                        \n",
        "\n",
        "train_upsampled = pd.concat([wa, rte_upsampled, ac_upsampled, tle_upsampled, mle_upsampled])\n",
        "\n",
        "X_train, y_train = train_upsampled.drop(columns=['Total','Status']), train_upsampled['Status']\n",
        "X_test, y_test = test.drop(columns=['Total','Status']), test['Status']\n",
        "\n",
        "#scaling features\n",
        "scaler = preprocessing.StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "#convert to dataframe\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYLV30yxdqv0"
      },
      "source": [
        "Status is grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUWc7Xeudqv0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "\n",
        "train, test = train_test_split(df_encode, test_size=0.2, stratify=df_encode['Status'].values)\n",
        "\n",
        "# grouping status\n",
        "# 2 wa    1945\n",
        "# 1 er    992\n",
        "# 0 ac     386\n",
        "\n",
        "#upsampling the minority classes Accepted and Errors\n",
        "ac = train[train.Status==0]\n",
        "er = train[train.Status==1]\n",
        "wa = train[train.Status==2]\n",
        "\n",
        "ac_upsampled = resample(ac, replace=True, # sample with replacement\n",
        "                        n_samples=len(wa)) # match number in majority class\n",
        "                        \n",
        "\n",
        "er_upsampled = resample(er, replace=True,\n",
        "                        n_samples=len(wa))\n",
        "                    \n",
        "\n",
        "train_upsampled = pd.concat([wa, er_upsampled, ac_upsampled])\n",
        "\n",
        "X_train, y_train = train_upsampled.drop(columns=['Total','Status']), train_upsampled['Status']\n",
        "X_test, y_test = test.drop(columns=['Total','Status']), test['Status']\n",
        "\n",
        "#scaling features\n",
        "scaler = preprocessing.StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "#convert to dataframe\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPbdGqRrdqv0"
      },
      "source": [
        "# Prepare train and test data, using all features (textstat + tokenized features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eLJYKK7dqv0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "\n",
        "df_allfeatures_encode = df_tokenized.copy()\n",
        "le = preprocessing.LabelEncoder()\n",
        "df_allfeatures_encode.drop(columns=['Problem','Description'], inplace=True)\n",
        "# encode Status [Accepted, Error, Wrong Answer] to [0,1,2]\n",
        "df_allfeatures_encode['Status'] = le.fit_transform(df_allfeatures_encode['Status'])\n",
        "\n",
        "train_allfeatures, test_allfeatures = train_test_split(df_allfeatures_encode, test_size=0.2, stratify=df_allfeatures_encode['Status'].values)\n",
        "\n",
        "#upsampling the minority classes Accepted and Errors\n",
        "ac_all = train_allfeatures[train_allfeatures.Status==0]\n",
        "er_all = train_allfeatures[train_allfeatures.Status==1]\n",
        "wa_all = train_allfeatures[train_allfeatures.Status==2]\n",
        "\n",
        "ac_all_upsampled = resample(ac_all, replace=True, # sample with replacement\n",
        "                        n_samples=len(wa_all)) # match number in majority class\n",
        "                        \n",
        "\n",
        "er_all_upsampled = resample(er_all, replace=True,\n",
        "                        n_samples=len(wa_all))\n",
        "                    \n",
        "\n",
        "train_allfeatures_upsampled = pd.concat([wa_all, er_all_upsampled, ac_all_upsampled])\n",
        "\n",
        "X_allfeatures_train, y_allfeatures_train = train_allfeatures_upsampled.drop(columns=['Total','Status']), train_allfeatures_upsampled['Status']\n",
        "X_allfeatures_test, y_allfeatures_test = test_allfeatures.drop(columns=['Total','Status']), test_allfeatures['Status']\n",
        "\n",
        "#scaling features\n",
        "scaler_all = preprocessing.StandardScaler()\n",
        "X_allfeatures_train_scaled = scaler_all.fit_transform(X_allfeatures_train)\n",
        "X_allfeatures_test_scaled = scaler_all.transform(X_allfeatures_test)\n",
        "\n",
        "#convert to dataframe\n",
        "X_allfeatures_train_scaled = pd.DataFrame(X_allfeatures_train_scaled, columns=X_allfeatures_train.columns)\n",
        "X_allfeatures_test_scaled = pd.DataFrame(X_allfeatures_test_scaled, columns=X_allfeatures_test.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJbF_bYzdqv0"
      },
      "source": [
        "# Select K Best\n",
        "\n",
        "Only textstat features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay4DTVPFdqv0",
        "outputId": "54c07a6f-afe8-4b4f-ce89-e155de60f712"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
        "\n",
        "\n",
        "kBest = SelectKBest(score_func = f_classif, k=14)\n",
        "kBestFeatures = kBest.fit(X_train_scaled.values, y_train.values)\n",
        "\n",
        "dfscores = pd.DataFrame(kBestFeatures.scores_)\n",
        "dfcolumns = pd.DataFrame(X_train_scaled.columns)\n",
        "\n",
        "featureScores = pd.concat([dfcolumns, dfscores],axis=1)\n",
        "featureScores.columns = ['Feature','Score']\n",
        "print(featureScores.nlargest(20,'Score'))\n",
        "\n",
        "top_12 = featureScores.nlargest(12, 'Score')\n",
        "print(\"top 12 scores: \\n\", top_12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "AyNgw8Aadqv1",
        "outputId": "258c162e-b2c4-4425-915a-ab4f2018317a"
      },
      "outputs": [],
      "source": [
        "# Extract the feature names and scores\n",
        "top_12_feature_names = top_12['Feature'].to_numpy()\n",
        "top_12_scores = top_12['Score'].to_numpy()\n",
        "\n",
        "# Extract the top 10 features from the scaled training data\n",
        "X_train_scaled_top_12 = X_train_scaled[top_12_feature_names]\n",
        "plt.bar(top_12_feature_names, top_12_scores)\n",
        "\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel('Feature', fontsize=12)\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.title('Top 12 Feature Scores', fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDtHhaIpdqv1",
        "outputId": "055705a8-92cb-4386-df2c-4689e46cafae"
      },
      "outputs": [],
      "source": [
        "cc = pd.concat([X_train_scaled_top_12,y_train.reset_index(drop=True)], axis = 1)\n",
        "sns.pairplot(cc, hue='Status')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX9jxgL-Jc7R"
      },
      "source": [
        "# K-means\n",
        "\n",
        "Only textstat features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98OVuIXyJead"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "km = KMeans()\n",
        "visualizer = KElbowVisualizer(km, k=(2,30))\n",
        "\n",
        "visualizer.fit(X_train_scaled_top_12)        \n",
        "visualizer.show()        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BECurTp8dqv1"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "wcss = []\n",
        "for i in range(1, 31):\n",
        "    km = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 30)\n",
        "    km.fit(X_train_scaled_top_12)\n",
        "    wcss.append(km.inertia_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtpdXrk-dqv1",
        "outputId": "65f5f3d3-603e-4cba-f983-c3c48be82a82"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1, 31), wcss, marker = 'o')\n",
        "plt.xticks(range(1, 31, 1))\n",
        "plt.title('The Elbow Method')\n",
        "plt.xlabel('No. of Clusters')\n",
        "plt.ylabel('wcss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb3056XZdqv2"
      },
      "outputs": [],
      "source": [
        "#X_train_scaled_top_12_Kmeans = X_train_scaled_top_12.drop(columns=['Pass', 'Unpass'])\n",
        "#concat training features and label\n",
        "kmeans_Xtrain = pd.concat([X_train_scaled_top_12, pd.DataFrame(y_train).reset_index(drop=True)], axis=1)\n",
        "kmeans_Xtrain.drop(columns=['Pass', 'Unpass'], inplace=True)\n",
        "kmeans = KMeans(n_clusters = 3)\n",
        "kmeans.fit(kmeans_Xtrain)\n",
        "y_kmeans = kmeans.predict(kmeans_Xtrain)\n",
        "#X.drop(columns=['prediction'], inplace=True)\n",
        "X_train_scaled_top_12_K_means = kmeans_Xtrain.copy()\n",
        "X_train_scaled_top_12_K_means['K_means_prediction'] = y_kmeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "RYinfzEivMf6",
        "outputId": "c676d622-8472-481a-f946-aee8e9a61502"
      },
      "outputs": [],
      "source": [
        "from pandas.plotting import parallel_coordinates\n",
        "from itertools import cycle, islice\n",
        "\n",
        "my_colors = list(islice(cycle(['b', 'r', 'g']), None, len(y_kmeans)))\n",
        "plt.figure(figsize=(18,5))\n",
        "centroids =  pd.DataFrame(kmeans.cluster_centers_)\n",
        "centroid_labels = kmeans.predict(centroids)\n",
        "centroids['K_means_prediction'] = centroid_labels\n",
        "centroids.columns = X_train_scaled_top_12_K_means.columns\n",
        "parallel_coordinates(centroids, 'K_means_prediction', color = my_colors, marker='o')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbXGx4P1dqv2"
      },
      "source": [
        "# Select k Best using all features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFbkelgwdqv2",
        "outputId": "aa3b98ea-5ea1-479c-a6df-9acbdc490ff7"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
        "\n",
        "X_allfeatures_train_scaled_KBest = X_allfeatures_train_scaled #.drop(columns=['Pass', 'Unpass'])\n",
        "kBest_all = SelectKBest(score_func = f_classif, k=14)\n",
        "kBestFeatures_all = kBest_all.fit(X_allfeatures_train_scaled_KBest.values, y_allfeatures_train.values)\n",
        "\n",
        "dfscores_all = pd.DataFrame(kBestFeatures_all.scores_)\n",
        "dfcolumns_all = pd.DataFrame(X_allfeatures_train_scaled_KBest.columns)\n",
        "\n",
        "featureScores_all = pd.concat([dfcolumns_all, dfscores_all],axis=1)\n",
        "featureScores_all.columns = ['Feature','Score']\n",
        "print(featureScores_all.nlargest(150,'Score'))\n",
        "\n",
        "top_150_all = featureScores_all.nlargest(150, 'Score')\n",
        "#print(\"top 12 scores: \\n\", top_12_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "6RIoUz8Qdqv2",
        "outputId": "0971b7ab-c9c9-4605-9108-dfa7df44d0fe"
      },
      "outputs": [],
      "source": [
        "# Extract the feature names and scores\n",
        "top_150_feature_names_all = top_150_all['Feature'].to_numpy()\n",
        "top_150_scores_all = top_150_all['Score'].to_numpy()\n",
        "\n",
        "X_allfeatures_train_scaled_top150 = X_allfeatures_train_scaled[top_150_feature_names_all]\n",
        "plt.bar(top_150_feature_names_all, top_150_scores_all)\n",
        "\n",
        "\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel('Feature', fontsize=12)\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.title('Top 150 Feature Scores', fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3sV9OMz7CAi"
      },
      "source": [
        "# PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bzsnOtNdqv3"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA().fit(X_allfeatures_train_scaled_top150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "ATpFg6Qcdqv3",
        "outputId": "3242c4da-2454-4d86-b7da-02440eb36fc3"
      },
      "outputs": [],
      "source": [
        "plt.bar(range(1, 151), pca.explained_variance_ratio_, alpha=0.5, align='center',\n",
        "       label='Individual explained variance')\n",
        "plt.step(range(1, 151), np.cumsum(pca.explained_variance_ratio_), where='mid',\n",
        "         label='Cumulative explained variance')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal component index')\n",
        "plt.legend(loc='best')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dM3RJZfLdqv3"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=60)\n",
        "X_train_pca = pca.fit_transform(X_allfeatures_train_scaled_top150)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "GGCHruIZdqv3",
        "outputId": "360af716-9677-45bc-c30d-a208aaee3418"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "mlp_param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (50,100), (50, 100, 50), (50, 100, 50, 10), (100,), (100, 200), (100, 200, 100), (100, 200, 100, 10), (200,), (200, 300), (200, 300, 100), (200, 300, 100, 10), (300,), (300, 400), (300, 400, 200), (300, 400, 200,10)], \n",
        "    'activation' : ['logistic', 'tanh', 'relu'], \n",
        "    'solver' : ['sgd', 'adam'], \n",
        "    'alpha': [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n",
        "    'learning_rate': ['adaptive'],\n",
        "    'early_stopping': [True],\n",
        "    'max_iter': [500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000],\n",
        "    'tol': [0.0001, 0.001, 0.01]  \n",
        "}\n",
        "\n",
        "mlp_grid_search = GridSearchCV(\n",
        "    estimator=MLPClassifier(),\n",
        "    param_grid=mlp_param_grid,\n",
        "    cv=10,  \n",
        "    n_jobs=-1,  \n",
        "    verbose=1,  \n",
        "    scoring='f1_micro'\n",
        ")\n",
        "\n",
        "mlp_grid_search.fit(X_train_pca, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieHJcmCHdqv3"
      },
      "outputs": [],
      "source": [
        "# test data\n",
        "X_allfeatures_test_scaled_top150 = X_allfeatures_test_scaled[top_150_feature_names_all]\n",
        "X_test_pca = pca.transform(X_allfeatures_test_scaled_top150)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NWVXxwyDBmy",
        "outputId": "0f697912-7a19-4b60-f33c-72a7202575b2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "mlp_best_params = mlp_grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", mlp_best_params)\n",
        "\n",
        "# Get the best model found by GridSearchCV\n",
        "mlp_best_model = mlp_grid_search.best_estimator_\n",
        "y_pred = mlp_best_model.predict(X_test_pca)\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(\"ANN Classification Report:\")\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)\n",
        "print(accuracy_score(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ckc2lC1Qdqv3"
      },
      "source": [
        "# Random forest model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "k7LQqZuCdqv4",
        "outputId": "c731aa3b-140e-4f91-cd68-f93f6092c67c"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Parameter grid\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [10, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000],\n",
        "    'max_depth': [None, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100],\n",
        "    'min_samples_split': [2, 5, 10, 15, 20],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "rf_grid_search = GridSearchCV(estimator=rf,\n",
        "                           param_grid=rf_param_grid,\n",
        "                           scoring='f1_micro',\n",
        "                           cv=10,\n",
        "                           n_jobs=-1,\n",
        "                           verbose=2)\n",
        "\n",
        "rf_grid_search.fit(X_train_pca, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaFbqtwLdqv4",
        "outputId": "06b8a8c0-e5fa-4c5a-d490-1a164fbc7a1f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "rf_best_params = rf_grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", rf_best_params)\n",
        "\n",
        "rf_best_model = rf_grid_search.best_estimator_\n",
        "y_pred = rf_best_model.predict(X_test_pca)\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(\"Random Forest Classification Report:\")\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)\n",
        "print(accuracy_score(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqPgkqwDdqv4"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkQUSpgBdqv4"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Parameter grid\n",
        "svc_param_grid = {\n",
        "    'C': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
        "    'gamma': [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
        "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
        "}\n",
        "\n",
        "svc = SVC()\n",
        "svc_grid_search = GridSearchCV(estimator=svc,\n",
        "                           param_grid=svc_param_grid,\n",
        "                           scoring='f1_micro',\n",
        "                           cv=10,\n",
        "                           n_jobs=-1, verbose=2)\n",
        "\n",
        "svc_grid_search.fit(X_train_pca, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBqtqeaOdqv4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "svc_best_params = svc_grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", svc_best_params)\n",
        "\n",
        "svc_best_model = svc_grid_search.best_estimator_\n",
        "y_pred = svc_best_model.predict(X_test_pca)\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(\"SVM Classification Report:\")\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)\n",
        "print(accuracy_score(y_test,y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "0338e61295d1aca02160253f0caf20412f867af2313189bd3cee902ac797b084"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
